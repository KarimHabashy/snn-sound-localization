@article{Grothe2010,
	title        = {Mechanisms of Sound Localization in Mammals},
	author       = {Benedikt Grothe and Michael Pecka and David Mcalpine},
	year         = {2010},
	doi          = {10.1152/physrev.00026.2009.-The},
	url          = {www.prv.org}
}
@generic{Grothe2014,
	title        = {The natural history of sound localization in mammals-a story of neuronal inhibition},
	author       = {Benedikt Grothe and Michael Pecka},
	year         = {2014},
	month        = {10},
	journal      = {Frontiers in Neural Circuits},
	publisher    = {Frontiers Media S.A.},
	volume       = {8},
	doi          = {10.3389/fncir.2014.00116},
	issn         = {16625110},
	abstract     = {Our concepts of sound localization in the vertebrate brain are widely based on the general assumption that both the ability to detect air-borne sounds and the neuronal processing are homologous in archosaurs (present day crocodiles and birds) and mammals. Yet studies repeatedly report conflicting results on the neuronal circuits and mechanisms, in particular the role of inhibition, as well as the coding strategies between avian and mammalian model systems. Here we argue that mammalian and avian phylogeny of spatial hearing is characterized by a convergent evolution of hearing air-borne sounds rather than by homology. In particular, the different evolutionary origins of tympanic ears and the different availability of binaural cues in early mammals and archosaurs imposed distinct constraints on the respective binaural processing mechanisms. The role of synaptic inhibition in generating binaural spatial sensitivity in mammals is highlighted, as it reveals a unifying principle of mammalian circuit design for encoding sound position. Together, we combine evolutionary, anatomical and physiological arguments for making a clear distinction between mammalian processing mechanisms and coding strategies and those of archosaurs. We emphasize that a consideration of the convergent nature of neuronal mechanisms will significantly increase the explanatory power of studies of spatial processing in both mammals and birds.},
	issue        = {OCT},
	keywords     = {Archosaurs,Binaural hearing,Birds,Evolution,GABA,Glycine,LSO,MSO},
	pmid         = {25324726}
}
@article{Yin2019,
	title        = {Neural mechanisms of binaural processing in the auditory brainstem},
	author       = {Tom C.T. Yin and Phil H. Smith and Philip X. Joris},
	year         = {2019},
	month        = {10},
	journal      = {Comprehensive Physiology},
	publisher    = {Wiley-Blackwell Publishing Ltd},
	volume       = {9},
	pages        = {1503--1575},
	doi          = {10.1002/cphy.c180036},
	issn         = {20404603},
	abstract     = {Spatial hearing, and more specifically the ability to localize sounds in space, is one of the most studied and best understood aspects of hearing. Because there is no coding of acoustic space at the receptor organ, physiological sensitivity to spatial aspects of sounds first emerges in the central nervous system. Much progress has been made in the identification and characterization of the circuits in the auditory brainstem that create sensitivity to binaural and monaural cues toward acoustic space. We review the progress over the past third of a century, with a focus on the mammalian brainstem and on the anatomy and cellular physiology underlying the physiological tuning of monaural and binaural circuits to acoustic cues toward spatial hearing. In addition to examining the detailed mechanisms involved in the processing of the three main spatial cues, we also review the integration of these cues and their use toward behavior. © 2019 American Physiological Society.},
	issue        = {4},
	pmid         = {31688966}
}
@conference{Kluyver2016jupyter,
	title        = {Jupyter Notebooks -- a publishing format for reproducible computational workflows},
	author       = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\'a}n Avila and Safia Abdalla and Carol Willing},
	year         = {2016},
	booktitle    = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
	pages        = {87--90},
	doi          = {10.3233/978-1-61499-649-1-87},
	editor       = {F. Loizides and B. Schmidt},
	organization = {IOS Press}
}
@article{Zenke2018,
	title = {{SuperSpike}: {Supervised} learning in multilayer spiking neural networks},
	issn = {1530888X},
	doi = {10.1162/neco_a_01086},
	abstract = {A vast majority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in-vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in-silico. Here we revisit the problem of supervised learning in temporally coding multi-layer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three factor learning rule capable of training multi-layer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike-time patterns.},
	journal = {Neural Computation},
	author = {Friedemann Zenke and Surya Ganguli},
	year = {2018},
}
@article{durlach_equalization_1963,
	title = {Equalization and {Cancellation} {Theory} of {Binaural} {Masking}‐{Level} {Differences}},
	volume = {35},
	issn = {0001-4966},
	url = {https://doi.org/10.1121/1.1918675},
	doi = {10.1121/1.1918675},
	abstract = {In this paper, a quantitative “black‐box” model is developed for use in interpreting certain data on binaural‐masking‐level differences. The basic idea of this model is that the auditory system attempts to eliminate the masking components by first transforming the stimuli presented to the two ears so as to equalize the two masking components, and then subtracting. In order to obtain results that are quantitatively realistic, this processing is assumed to be corrupted by various types of errors. The model is applied to data in which the signal to be detected consists of a pulsed tone, the masking signal consists of loud, broad‐band, Gaussian noise, and the only differences between the stimuli presented to the two ears are those of time delay or amplitude (the interaural amplitude ratios being restricted to unity or zero). The results indicate that the ability of the auditory. system to control interaural intensity ratios and interaural time delays is limited to accuracies of about 1 dB and 150 μsec and that the auditory system has difficulty in compensating for interaural time delays greater than the time it takes for sound to travel a distance equal to the width of the head.},
	number = {8},
	urldate = {2024-06-20},
	journal = {The Journal of the Acoustical Society of America},
	author = {Durlach, N. I.},
	month = aug,
	year = {1963},
	pages = {1206--1218},
	file = {Snapshot:C\:\\Users\\dgoodman\\Zotero\\storage\\SFVPTPCH\\Equalization-and-Cancellation-Theory-of-Binaural.html:text/html},
}

@inproceedings{culling_equalization-cancellation_2020,
	address = {Lyon, France},
	title = {Equalization-cancellation revisited},
	url = {https://hal.science/hal-03234187},
	doi = {10.48465/fa.2020.0189},
	abstract = {Binaural masking level differences (BMLDs) are improvements in the masked detection thresholds of sounds that require binaural listening and are not attributable to improvements in signal-to-noise ratio. A BMLD can occur when there are differences in interaural phase, delay or level between target and masker. Equalization cancellation (EC) theory has long been a popular framework for explaining BMLDs. Durlach (1963, 1972) developed EC theory mathematically, predicting the results of each experiment using specific equations. His EC theory is admirably simple in concept and has only two fitted parameters, yet it generates many accurate predictions and invariably captures the general pattern of the data. However, some predictions remain grossly inaccurate or ambiguous. Two adapted EC models will be presented. One is a hybrid model that derives internal representations of the speech and noise computationally and then uses a single equation to predict the BMLD. The other is completely computational. The models include broadened auditory filtering, a cost function for the use of large internal delays, and, in the case for the computational model, peripheral non-linearities. The models will be used to demonstrate that a cost function and peripheral non-linearities are essential to provide accurate predictions of classic results in the literature. Broadened auditory filters have some explanatory power, but their role is indistinguishable with broadened from that of delay cost.},
	urldate = {2024-06-20},
	booktitle = {Forum {Acusticum}},
	author = {Culling, John},
	month = dec,
	year = {2020},
	keywords = {binaural, masking, model},
	pages = {1913--1917},
	file = {HAL PDF Full Text:C\:\\Users\\dgoodman\\Zotero\\storage\\FSQKN77X\\Culling - 2020 - Equalization-cancellation revisited.pdf:application/pdf},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	doi = {10.48550/arXiv.1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv:1912.01703 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
	annote = {Comment: 12 pages, 3 figures, NeurIPS 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\dgoodman\\Zotero\\storage\\7FM5S6K5\\Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\dgoodman\\Zotero\\storage\\DTU4ZCQ8\\1912.html:text/html},
}

@misc{zenke_spytorch_2019,
	title = {{SpyTorch}},
	url = {https://zenodo.org/records/3724018},
	abstract = {Tutorial-style PyTorch code packed into a bunch of Jupyter notebooks that give you everything you need to start with surrogate gradient learning in spiking neural networks.},
	urldate = {2024-06-20},
	publisher = {Zenodo},
	author = {Zenke, Friedemann},
	month = mar,
	year = {2019},
	doi = {10.5281/zenodo.3724018},
	keywords = {pytorch, spiking neural networks, supervised learning, surrogate gradients},
	file = {Snapshot:C\:\\Users\\dgoodman\\Zotero\\storage\\6U5BG6U4\\3724018.html:text/html},
}
@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}
@Article{Hunter2007,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}