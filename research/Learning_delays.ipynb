{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "YAW7y1InNQz1",
   "metadata": {
    "id": "YAW7y1InNQz1"
   },
   "source": [
    "# Learning delays v01\n",
    "\n",
    "I will start with the concept as shown in the below figure.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "![](diagrams/learning_delays_v01_concept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UnQonswnyP22",
   "metadata": {
    "id": "UnQonswnyP22"
   },
   "source": [
    "Method A: the aim is to pad the output spikes with a tensor of zeros then apply a delay. This will shift the spikes as shown in the figure, then the data will be surronded by zeros from both sides. However, this method will lead to misalignment between different anfs as some fibers will have the zero pad while others data at the same time point, which is not the case we expect. A possible solution is to clib the data as shown in Method B. I think, this will solve the problem, but will use a larger duration_steps. \n",
    "\n",
    "A way to implement this is shown below in the code. I think, this is not the only way and there is a paper that implemented the delays differently with a dedicated module: https://arxiv.org/abs/2205.02115. I have removed Dan's comments (sorry Dan) and some analysis to just focus on the delays.\n",
    "\n",
    "\n",
    "Here is my thoughts on the topic if anyone is interested, if not, it is skippable :). I think, delays (given they have sufficient resolution) in their essence, and for the first layer, are a transformer from the time domain to the spatial domain. For a fixed duration_steps and after application of delays, all information is avilable in the first layer to do any computation on the information in this step. To learn far away concepts in the time domain, we either increase the max delays range in the first layer or apply a RNN.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee3c91b7",
   "metadata": {
    "id": "ee3c91b7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "my_computer_is_slow = True # set this to True if using Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cG8QPdEaDFUb",
   "metadata": {
    "id": "cG8QPdEaDFUb"
   },
   "source": [
    "Comment #1 If we set the no_phase_delays flag to 'True', some how the network learns and the loss is reduced. It is not as good as with phase_delays. I just find it weird. I think, it might be good to decouple and the study the effects of weights and delays separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5bb26693",
   "metadata": {
    "id": "5bb26693"
   },
   "outputs": [],
   "source": [
    "# Not using Brian so we just use these constants to make equations look nicer below\n",
    "second = 1\n",
    "ms = 1e-3\n",
    "Hz = 1\n",
    "\n",
    "\"\"\"Delay paramters\"\"\"\n",
    "max_delay = 50*ms    # maximum allowed axonal delay during training\n",
    "trainable_delays, no_phase_delays = True, False\n",
    "num_ear = 2          # Who knows maybe there is an alien out there with more than 2 ears\n",
    "lr_delay = 1e-3      # Not fine tuned much, but improves performance on no_delays case\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "# Stimulus and simulation parameters\n",
    "dt = 1*ms            # large time step to make simulations run faster for tutorial\n",
    "anf_per_ear = 100    # repeats of each ear with independent noise\n",
    "envelope_power = 2   # higher values make sharper envelopes, easier\n",
    "rate_max = 600*Hz   # maximum Poisson firing rate\n",
    "f = 20*Hz            # stimulus frequency\n",
    "duration = .1*second # stimulus duration\n",
    "duration_steps = int(np.round(duration/dt))\n",
    "input_size = 2*anf_per_ear\n",
    "\n",
    "# Generate an input signal (spike array) from array of true IPDs\n",
    "def input_signal(ipd):\n",
    "    num_samples = len(ipd)\n",
    "    T = np.arange(duration_steps)*dt # array of times\n",
    "    phi = 2*np.pi*(f*T+np.random.rand()) # array of phases corresponding to those times with random offset\n",
    "    # each point in the array will have a different phase based on which ear it is\n",
    "    # and its delay\n",
    "    theta = np.zeros((num_samples, duration_steps, 2*anf_per_ear))\n",
    "    # for each ear, we have anf_per_ear different phase delays from to pi/2 so\n",
    "    # that the differences between the two ears can cover the full range from -pi/2 to pi/2\n",
    "    \"\"\"Delay related update\"\"\"\n",
    "    if trainable_delays or no_phase_delays:\n",
    "        phase_delays = np.array([0]*anf_per_ear)\n",
    "    else:\n",
    "        phase_delays = np.linspace(0, np.pi/2, anf_per_ear)\n",
    "    \"\"\"\"\"\"\n",
    "    # now we set up these theta to implement that. Some numpy vectorisation logic here which looks a little weird,\n",
    "    # but implements the idea in the text above.\n",
    "    theta[:, :, :anf_per_ear] = phi[np.newaxis, :, np.newaxis]+phase_delays[np.newaxis, np.newaxis, :]\n",
    "    theta[:, :, anf_per_ear:] = phi[np.newaxis, :, np.newaxis]+phase_delays[np.newaxis, np.newaxis, :]+ipd[:, np.newaxis, np.newaxis]\n",
    "    # now generate Poisson spikes at the given firing rate as in the previous notebook\n",
    "    spikes = np.random.rand(num_samples, duration_steps, 2*anf_per_ear)<rate_max*dt*(0.5*(1+np.sin(theta)))**envelope_power\n",
    "    return spikes\n",
    "\n",
    "# Generate some true IPDs from U(-pi/2, pi/2) and corresponding spike arrays\n",
    "def random_ipd_input_signal(num_samples, tensor=True):\n",
    "    ipd = np.random.rand(num_samples)*np.pi-np.pi/2 # uniformly random in (-pi/2, pi/2)\n",
    "    spikes = input_signal(ipd)\n",
    "    if tensor:\n",
    "        ipd = torch.tensor(ipd, device=device, dtype=dtype)        \n",
    "        spikes = torch.tensor(spikes, device=device, dtype=dtype)\n",
    "    return ipd, spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z01IZeWmEZas",
   "metadata": {
    "id": "Z01IZeWmEZas"
   },
   "source": [
    "Comment #2 The main actors that made the differentiable delay possible are the 'F.affine_grid' and 'F.grid_sample' functions. They provided ways to shift the delays while keeping the input spike shape and content. These functions are used in spatial transformers. Togther, I think, they make a soft function by interpolating between the indices. Delays are index related and hence they are not differentiable, so these functions are one way of circumventing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "DmMYVBk43M2P",
   "metadata": {
    "id": "DmMYVBk43M2P"
   },
   "outputs": [],
   "source": [
    "\"\"\"Delay related functions\"\"\"\n",
    "\n",
    "# Delays with constant or random initialisation\n",
    "# Might think of other ways to initialize delays and their effect on performance\n",
    "def init_delay_vector(constant=True, constant_value=0):\n",
    "    if constant:\n",
    "        delays = torch.nn.parameter.Parameter\\\n",
    "            (torch.FloatTensor([constant_value*ms]*anf_per_ear*num_ear), requires_grad=True)\n",
    "    else:\n",
    "        delays_numpy = np.random.randint(dt/ms, high=int(max_delay/ms), size=int(anf_per_ear*num_ear), dtype=int)*ms\n",
    "        delays = torch.nn.parameter.Parameter(torch.FloatTensor(delays_numpy), requires_grad=True)\n",
    "    return delays\n",
    "\n",
    "delays_out = init_delay_vector(constant=False)\n",
    "\n",
    "def apply_delays(spikes_in, delays):\n",
    "    # Limiting applied delays to the range(0, max_delay)\n",
    "    delays = torch.clamp(delays, min=0, max=max_delay - ms) / ms\n",
    "    # Expand the dimenion as required by the functions\n",
    "    spikes_in = spikes_in[:, None, :, :]\n",
    "    spikes_size = spikes_in.size()\n",
    "    zero_pad = torch.zeros((spikes_size[0], spikes_size[1], int(max_delay/ms), spikes_size[3]))\n",
    "    # Applying the zero pad\n",
    "    spikes_in = torch.cat((zero_pad, spikes_in), 2)\n",
    "    spikes_in = torch.transpose(spikes_in, 0, 3)\n",
    "    spikes_size = spikes_in.size()\n",
    "    batch_size_in, height, width = spikes_size[0], spikes_size[2], spikes_size[3]\n",
    "    # Applying a transformation with only translation\n",
    "    affine_transformation = torch.zeros(batch_size_in, 2, 3)\n",
    "    affine_transformation[:, 0, 0] = 1\n",
    "    affine_transformation[:, 1, 1] = 1\n",
    "    affine_transformation[:, 1, 2] = (2/height)*delays  # This is very important: delays have to be whole numbers or they will alter the input spikes values and distribution.\n",
    "    grid = F.affine_grid(affine_transformation, (batch_size_in, 1, height, width), align_corners=False)\n",
    "    # Grid sampling to acquire the shifted spikes\n",
    "    spikes_out = F.grid_sample(spikes_in, grid, align_corners=False)\n",
    "    spikes_out = torch.transpose(spikes_out, 0, 3)\n",
    "    spikes_out = torch.squeeze(spikes_out)\n",
    "    return spikes_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3f817078",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f817078",
    "outputId": "b1ee88fa-ed06-4781-ace0-159ef4e16e01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes = 12\n"
     ]
    }
   ],
   "source": [
    "# classes at 15 degree increments\n",
    "num_classes = 180//15\n",
    "print(f'Number of classes = {num_classes}')\n",
    "\n",
    "def discretise(ipds):\n",
    "    return ((ipds+np.pi/2)*num_classes/np.pi).long() # assumes input is tensor\n",
    "\n",
    "def continuise(ipd_indices): # convert indices back to IPD midpoints\n",
    "    return (ipd_indices+0.5)/num_classes*np.pi-np.pi/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c7c60d9",
   "metadata": {
    "id": "7c7c60d9"
   },
   "outputs": [],
   "source": [
    "# Weights and uniform weight initialisation\n",
    "def init_weight_matrix():\n",
    "    # Note that the requires_grad=True argument tells PyTorch that we'll be computing gradients with\n",
    "    # respect to the values in this tensor and thereby learning those values. If you want PyTorch to\n",
    "    # learn some gradients, make sure it has this on.\n",
    "    W = nn.Parameter(torch.empty((input_size, num_classes), device=device, dtype=dtype, requires_grad=True))\n",
    "    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(W)\n",
    "    bound = 1 / np.sqrt(fan_in)\n",
    "    nn.init.uniform_(W, -bound, bound)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fc5fdc4f",
   "metadata": {
    "id": "fc5fdc4f"
   },
   "outputs": [],
   "source": [
    "# Parameters for training. These aren't optimal, but instead designed\n",
    "# to give a reasonable result in a small amount of time for the tutorial!\n",
    "if my_computer_is_slow:\n",
    "    batch_size = 64\n",
    "    n_training_batches = 64\n",
    "else:\n",
    "    batch_size = 128\n",
    "    n_training_batches = 128\n",
    "n_testing_batches = 32\n",
    "num_samples = batch_size*n_training_batches\n",
    "\n",
    "# Generator function iterates over the data in batches\n",
    "# We randomly permute the order of the data to improve learning\n",
    "def data_generator(ipds, spikes):\n",
    "    perm = torch.randperm(spikes.shape[0])\n",
    "    spikes = spikes[perm, :, :]\n",
    "    ipds = ipds[perm]\n",
    "    n, _, _ = spikes.shape\n",
    "    n_batch = n//batch_size\n",
    "    for i in range(n_batch):\n",
    "        x_local = spikes[i*batch_size:(i+1)*batch_size, :, :]\n",
    "        y_local = ipds[i*batch_size:(i+1)*batch_size]\n",
    "        yield x_local, y_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e5fabc7b",
   "metadata": {
    "id": "e5fabc7b"
   },
   "outputs": [],
   "source": [
    "beta = 5\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        # Original SPyTorch/SuperSpike gradient\n",
    "        # This seems to be a typo or error? But it works well\n",
    "        #grad = grad_output/(100*torch.abs(input)+1.0)**2\n",
    "        # Sigmoid\n",
    "        grad = grad_output*beta*torch.sigmoid(beta*input)*(1-torch.sigmoid(beta*input))\n",
    "        return grad\n",
    "\n",
    "spike_fn  = SurrGradSpike.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b072bb5",
   "metadata": {
    "id": "7b072bb5"
   },
   "outputs": [],
   "source": [
    "num_hidden = 30\n",
    "\n",
    "# Weights and uniform weight initialisation\n",
    "def init_weight_matrices():\n",
    "    # Input to hidden layer\n",
    "    W1 = nn.Parameter(torch.empty((input_size, num_hidden), device=device, dtype=dtype, requires_grad=True))\n",
    "    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(W)\n",
    "    bound = 1 / np.sqrt(fan_in)\n",
    "    nn.init.uniform_(W1, -bound, bound)\n",
    "    # Hidden layer to output\n",
    "    W2 = nn.Parameter(torch.empty((num_hidden, num_classes), device=device, dtype=dtype, requires_grad=True))\n",
    "    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(W)\n",
    "    bound = 1 / np.sqrt(fan_in)\n",
    "    nn.init.uniform_(W2, -bound, bound)\n",
    "    return W1, W2\n",
    "\n",
    "# Run the simulation\n",
    "def snn(input_spikes, W1, W2, delays, tau=20*ms):\n",
    "    \"\"\"Delay related update\"\"\"\n",
    "    if trainable_delays:\n",
    "        input_spikes = apply_delays(input_spikes, delays)\n",
    "        duration_in = duration_steps + int(max_delay / ms)\n",
    "    else:\n",
    "        duration_in = duration_steps\n",
    "    \"\"\"\"\"\"\n",
    "    # First layer: input to hidden\n",
    "    v = torch.zeros((batch_size, num_hidden), device=device, dtype=dtype)\n",
    "    s = torch.zeros((batch_size, num_hidden), device=device, dtype=dtype)\n",
    "    s_rec = [s]\n",
    "    h = torch.einsum(\"abc,cd->abd\", (input_spikes, W1))\n",
    "    alpha = np.exp(-dt/tau)\n",
    "    \"\"\"Delay related update\"\"\"\n",
    "    for t in range(duration_in - 1):\n",
    "        new_v = (alpha*v + h[:, t, :])*(1-s) # multiply by 0 after a spike\n",
    "        s = spike_fn(v-1) # threshold of 1\n",
    "        v = new_v\n",
    "        s_rec.append(s)\n",
    "    s_rec = torch.stack(s_rec, dim=1)\n",
    "    # Second layer: hidden to output\n",
    "    v = torch.zeros((batch_size, num_classes), device=device, dtype=dtype)\n",
    "    s = torch.zeros((batch_size, num_classes), device=device, dtype=dtype)\n",
    "    v_rec = [v]\n",
    "    h = torch.einsum(\"abc,cd->abd\", (s_rec, W2))\n",
    "    alpha = np.exp(-dt/tau)\n",
    "    \"\"\"Delay related update\"\"\"\n",
    "    for t in range(duration_in - 1):\n",
    "        v = alpha*v + h[:, t, :]\n",
    "        v_rec.append(v)\n",
    "    v_rec = torch.stack(v_rec, dim=1)\n",
    "    # Return recorded membrane potential of output\n",
    "    return v_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gfY2nQXjHeGi",
   "metadata": {
    "id": "gfY2nQXjHeGi"
   },
   "source": [
    "Comment #3 The next issue, after the differntiable delays, is the application of the gradients. The gradients need to be within a reasonable range (a fraction of the maximum delay) and a multiple of the dt (minimum delay). For this reason I used SGD with 0 parameters so that the update is: delays_new = delays_old + lr_delay*gradients. I think a dedicated optimizer might be needed to provide a robust and safe way to apply the gradients to ensure that the updated delays are always whole numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5d558df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "a5d558df",
    "outputId": "885c455a-db05-4eb4-99b0-db71d5a86ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Want loss for epoch 1 to be about 2.48, multiply m by constant to get this\n",
      "Epoch 1: loss=2.32046\n",
      "Epoch 2: loss=1.63760\n",
      "Epoch 3: loss=1.41692\n",
      "Epoch 4: loss=1.25174\n",
      "Epoch 5: loss=1.03296\n",
      "Epoch 6: loss=0.92214\n",
      "Epoch 7: loss=0.81613\n",
      "Epoch 8: loss=0.78015\n",
      "Epoch 9: loss=0.71864\n",
      "Epoch 10: loss=0.69677\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training parameters\n",
    "nb_epochs = 10 # quick, it won't have converged\n",
    "lr = 0.01 # learning rate\n",
    "\n",
    "# Generate the training data\n",
    "ipds, spikes = random_ipd_input_signal(num_samples)\n",
    "\n",
    "# Initialise a weight matrix\n",
    "W = init_weight_matrix()\n",
    "# Initialise a weight matrices\n",
    "W1, W2 = init_weight_matrices()\n",
    "\n",
    "# Optimiser and loss function\n",
    "optimizer = torch.optim.Adam([W1, W2], lr=lr)\n",
    "log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\"\"\"Delay related update\"\"\"\n",
    "optimizer_delay = torch.optim.SGD([delays_out], lr=lr_delay, weight_decay=0, momentum=0)\n",
    "\n",
    "print(f\"Want loss for epoch 1 to be about {-np.log(1/num_classes):.2f}, multiply m by constant to get this\")\n",
    "\n",
    "loss_hist = []\n",
    "for e in range(nb_epochs):\n",
    "    local_loss = []\n",
    "    for x_local, y_local in data_generator(discretise(ipds), spikes):\n",
    "        # Run network\n",
    "        \"\"\"Delay related update\"\"\"\n",
    "        # Apply the delays only to the input spikes\n",
    "        output = snn(x_local, W1, W2, delays_out)\n",
    "        \"\"\"\"\"\"\n",
    "        # Compute cross entropy loss\n",
    "        m = torch.mean(output, 1)  # Mean across time dimension\n",
    "        loss = loss_fn(log_softmax_fn(m), y_local)\n",
    "        local_loss.append(loss.item())\n",
    "        # Update gradients\n",
    "        \"\"\"Delay related update\"\"\"\n",
    "        if trainable_delays:\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_delay.zero_grad()\n",
    "            loss.backward()\n",
    "            #  This is a crucial part, controlling the applied gradients is important to ensure that the updated delays are whole numbers.\n",
    "            delays_out.grad = torch.round(delays_out.grad)  # Might need optimizing like applying a clip function\n",
    "            optimizer_delay.step()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \"\"\"\"\"\"\n",
    "\n",
    "    loss_hist.append(np.mean(local_loss))\n",
    "    print(\"Epoch %i: loss=%.5f\"%(e+1, np.mean(local_loss)))\n",
    "\n",
    "# Plot the loss function over time\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voVcUWqSIdH-",
   "metadata": {
    "id": "voVcUWqSIdH-"
   },
   "source": [
    "Comment #4: This is a protoype of what I think might work and needs more work. Here is a list of things I think can be done regarding the work here:\n",
    "1) Check the code works as intended\n",
    "2) Code optimization to facilitate learning\n",
    "3) Apply Method B: shift + clip\n",
    "4) A dedicated optimizer or a robust and general way to update the gradients to ensure that the updated delays are whole numbers. The gradients are discrete and should be in multiples of dt. \n",
    "5) Maybe a dedicated delays optimizer?\n",
    "6) Other methods?\n",
    "\n",
    "Finally, comments, suggestions and questions would be welcome and helpful. I am not an expert in this area so this might be slow. If anyone is any expert, I don't mind handing over."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Starting-Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
